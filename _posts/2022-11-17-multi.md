## 다항논리회귀(Multinomial logistic regression)
와인의 도수, 산도, 색을 이용해서 3개의 클래스로 분류 

## 캐글 불러오고


```python
import os
os.environ['KAGGLE_USERNAME'] = 'seungnote' # username
os.environ['KAGGLE_KEY'] = 'bc254ae94554f39d41fcab91537514ad' # key
```


```python
!kaggle datasets download -d brynja/wineuci
```

    Downloading wineuci.zip to D:\anaconda\00.studying
    
    

    
      0%|          | 0.00/4.20k [00:00<?, ?B/s]
    100%|##########| 4.20k/4.20k [00:00<?, ?B/s]
    

## import 해주고


```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam, SGD
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
#추가된거 
from sklearn.preprocessing import OneHotEncoder
```

## 데이터 가져오기 및 정렬


```python
df=pd.read_csv("D:/anaconda/00.studying/data/Wine.csv")
df.head(5)
# 근데 header가 안나옴 대신 채워주기 
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>14.23</th>
      <th>1.71</th>
      <th>2.43</th>
      <th>15.6</th>
      <th>127</th>
      <th>2.8</th>
      <th>3.06</th>
      <th>.28</th>
      <th>2.29</th>
      <th>5.64</th>
      <th>1.04</th>
      <th>3.92</th>
      <th>1065</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>14.20</td>
      <td>1.76</td>
      <td>2.45</td>
      <td>15.2</td>
      <td>112</td>
      <td>3.27</td>
      <td>3.39</td>
      <td>0.34</td>
      <td>1.97</td>
      <td>6.75</td>
      <td>1.05</td>
      <td>2.85</td>
      <td>1450</td>
    </tr>
  </tbody>
</table>
</div>




```python
df=pd.read_csv("D:/anaconda/00.studying/data/Wine.csv",names=['name'
  ,'alcohol'
  ,'malicAcid'
  ,'ash'
  ,'ashalcalinity'
  ,'magnesium'
  ,'totalPhenols'
  ,'flavanoids'
  ,'nonFlavanoidPhenols'
  ,'proanthocyanins'
  ,'colorIntensity'
  ,'hue'
  ,'od280_od315'
  ,'proline'])
df.head(5)

# Y가 name 나머지가  ㅌ
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>alcohol</th>
      <th>malicAcid</th>
      <th>ash</th>
      <th>ashalcalinity</th>
      <th>magnesium</th>
      <th>totalPhenols</th>
      <th>flavanoids</th>
      <th>nonFlavanoidPhenols</th>
      <th>proanthocyanins</th>
      <th>colorIntensity</th>
      <th>hue</th>
      <th>od280_od315</th>
      <th>proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>



## 데이터 살펴보기 countplot+isnull


```python
#데이터 살펴보기

sns.countplot(x=df['name'])

# name의 종류가 3가지인걸 알수있음 
# 1 60개 2 70개 ...
```




    <AxesSubplot:xlabel='name', ylabel='count'>




    
![png](output_10_1.png)
    



```python
print(df.isnull().sum())
# 빈 행 찾기 ->0이니깐 아주 좋은거
```

    name                   0
    alcohol                0
    malicAcid              0
    ash                    0
    ashalcalinity          0
    magnesium              0
    totalPhenols           0
    flavanoids             0
    nonFlavanoidPhenols    0
    proanthocyanins        0
    colorIntensity         0
    hue                    0
    od280_od315            0
    proline                0
    dtype: int64
    

## 데이터 분할


```python
# 이제는 x y 데이터 분할 
x_data=df.drop(columns=['name'],axis=1)
x_data=x_data.astype(np.float32)
x_data.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>alcohol</th>
      <th>malicAcid</th>
      <th>ash</th>
      <th>ashalcalinity</th>
      <th>magnesium</th>
      <th>totalPhenols</th>
      <th>flavanoids</th>
      <th>nonFlavanoidPhenols</th>
      <th>proanthocyanins</th>
      <th>colorIntensity</th>
      <th>hue</th>
      <th>od280_od315</th>
      <th>proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.600000</td>
      <td>127.0</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.200000</td>
      <td>100.0</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.600000</td>
      <td>101.0</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.799999</td>
      <td>113.0</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.000000</td>
      <td>118.0</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
y_data=df[['name']]
y_data=y_data.astype(np.float32)
y_data.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>



## 데이터 표준화


```python
#정의하고
scaler = StandardScaler()
#호출 
x_data_scaled = scaler.fit_transform(x_data)
#표준화 전
print(x_data.values[0])
#표준화 후 
print(x_data_scaled[0])
```

    [1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00
     2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]
    [ 1.5186119  -0.5622497   0.2320528  -1.1695931   1.9139051   0.8089973
      1.0348189  -0.65956306  1.2248839   0.2517168   0.3621771   1.8479197
      1.013009  ]
    

## ★one-hot encoding

y데이터는 원래 1,2,3이 있었음

이걸 원핫으로 만들면 이렇게 됨 

1 [1 0 0]

2 [0 1 0]

3 [0 0 1]


```python
encoder=OneHotEncoder()
y_data_encoded= encoder.fit_transform(y_data).toarray()

# 전
print(y_data.values[0])
# 후 
print(y_data_encoded[0])
```

    [1.]
    [1. 0. 0.]
    

## 학습/검증 데이터 분할

차이점:
기존)x_train, x_val, y_train, y_val = train_test_split(**x_data, y_data,** test_size=0.2, random_state=2021)

변경)x_train, x_val, y_train, y_val = train_test_split(**x_data_scaled, y_data_encoded,** test_size=0.2, random_state=2021)


```python
x_train, x_val, y_train, y_val = train_test_split(x_data_scaled, y_data_encoded, test_size=0.2, random_state=2021)
print(x_train.shape, x_val.shape)
print(y_train.shape, y_val.shape)

# 학습 143개 validation 13개 (Feature의 갯수)
# 3-> y의 갯수 세 종류 
```

    (142, 13) (36, 13)
    (142, 3) (36, 3)
    

## 모델 학습

차이점: 

(1) softmax 

(2)Dense 3개

(3)binary 대신 categorical crossentropy 


```python
model = Sequential([
  Dense(3, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.02), metrics=['acc'])

model.fit(
    x_train,
    y_train,
    validation_data=(x_val, y_val), # 검증 데이터를 넣어주면 한 epoch이 끝날때마다 자동으로 검증
    epochs=20 # epochs 복수형으로 쓰기!
)

# 마지막에는 
# loss: 0.0546 - acc: 0.9930 - val_loss: 0.1223 - val_acc: 0.9444
#training 정확도 99% 
# validation 검증의 정확도는 94%

#y 데이터를 넣었을때 91.67%로 1,2,3번으로 구별할 수 있다 
```

    D:\anaconda\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
      super().__init__(name, **kwargs)
    

    Epoch 1/20
    5/5 [==============================] - 2s 69ms/step - loss: 1.3979 - acc: 0.4155 - val_loss: 1.1682 - val_acc: 0.5000
    Epoch 2/20
    5/5 [==============================] - 0s 7ms/step - loss: 0.9109 - acc: 0.5845 - val_loss: 0.7951 - val_acc: 0.6667
    Epoch 3/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.6080 - acc: 0.7465 - val_loss: 0.5641 - val_acc: 0.7778
    Epoch 4/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.4154 - acc: 0.8662 - val_loss: 0.4181 - val_acc: 0.8611
    Epoch 5/20
    5/5 [==============================] - 0s 7ms/step - loss: 0.3056 - acc: 0.9014 - val_loss: 0.3293 - val_acc: 0.9444
    Epoch 6/20
    5/5 [==============================] - 0s 10ms/step - loss: 0.2369 - acc: 0.9296 - val_loss: 0.2724 - val_acc: 0.9444
    Epoch 7/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.1917 - acc: 0.9366 - val_loss: 0.2355 - val_acc: 0.9444
    Epoch 8/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.1611 - acc: 0.9437 - val_loss: 0.2094 - val_acc: 0.9444
    Epoch 9/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.1382 - acc: 0.9648 - val_loss: 0.1902 - val_acc: 0.9444
    Epoch 10/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.1228 - acc: 0.9648 - val_loss: 0.1731 - val_acc: 0.9444
    Epoch 11/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.1099 - acc: 0.9718 - val_loss: 0.1603 - val_acc: 0.9444
    Epoch 12/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.0988 - acc: 0.9789 - val_loss: 0.1501 - val_acc: 0.9444
    Epoch 13/20
    5/5 [==============================] - 0s 9ms/step - loss: 0.0901 - acc: 0.9789 - val_loss: 0.1432 - val_acc: 0.9444
    Epoch 14/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.0820 - acc: 0.9859 - val_loss: 0.1370 - val_acc: 0.9444
    Epoch 15/20
    5/5 [==============================] - 0s 7ms/step - loss: 0.0763 - acc: 0.9859 - val_loss: 0.1330 - val_acc: 0.9444
    Epoch 16/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.0706 - acc: 0.9930 - val_loss: 0.1300 - val_acc: 0.9444
    Epoch 17/20
    5/5 [==============================] - 0s 7ms/step - loss: 0.0658 - acc: 0.9930 - val_loss: 0.1274 - val_acc: 0.9444
    Epoch 18/20
    5/5 [==============================] - 0s 7ms/step - loss: 0.0614 - acc: 0.9930 - val_loss: 0.1251 - val_acc: 0.9444
    Epoch 19/20
    5/5 [==============================] - 0s 7ms/step - loss: 0.0582 - acc: 0.9930 - val_loss: 0.1237 - val_acc: 0.9444
    Epoch 20/20
    5/5 [==============================] - 0s 8ms/step - loss: 0.0546 - acc: 0.9930 - val_loss: 0.1223 - val_acc: 0.9444
    




    <keras.callbacks.History at 0x2a4ef7a09a0>


