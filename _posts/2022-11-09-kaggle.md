## Kaggle 사용해보기

캐글에서 데이터셋 가져오기 

1. 가입 https://www.kaggle.com/

2. 계정>API>Create New API Token>다운

3. username과 key 값을 넣어준다 


```python
import os
os.environ['KAGGLE_USERNAME'] = 'seungnote' # username
os.environ['KAGGLE_KEY'] = 'bc254ae94554f39d41fcab91537514ad' # key
```

1. 광고 데이터셋 가져오기
 https://www.kaggle.com/ashydv/advertising-dataset
 
2. csv란 ,로 구분되어 있는거라는 뜻 

3. 각 매체별(TV,신문, 라디오) 투자 비용과 매출을 표현한 것 

4. **다운로드 방법
New Notebook 옆에 ..을 누르고 'Copy API command' 클릭**

5. 항상 !를 붙이고 실행 시켜야함 

※주의점
prompt에서 pip install kaggle을 해서 설치해주고,
C:kaggle폴더안에 내 username과 key값이 들어간 메모장을 넣어줘야한다 

6. 이제 기본 폴더에 들어가면 'advertising-dataset' zip이 들어가 있는걸 확인할 수있다. 

(문제)
!unzip /content/advertising-dataset.zip을 하면 압축 해제가 된다는데..작동이 안됨. 


```python
!kaggle datasets download -d ashydv/advertising-dataset
```

    Downloading advertising-dataset.zip to D:\anaconda\00.studying
    
    

    
      0%|          | 0.00/1.83k [00:00<?, ?B/s]
    100%|##########| 1.83k/1.83k [00:00<00:00, 624kB/s]
    


```python
#기본적으로 항상 씀

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam, SGD
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns

# 머신러닝을 도와주는 패키지. Training set과 Test set 분리해주는 기능 
from sklearn.model_selection import train_test_split
```


```python
df=pd.read_csv("D:/anaconda/00.studying/advertising-dataset/advertising.csv")
df.head(5)

# 미리보기로 체크해주는게 좋음 
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TV</th>
      <th>Radio</th>
      <th>Newspaper</th>
      <th>Sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>230.1</td>
      <td>37.8</td>
      <td>69.2</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>44.5</td>
      <td>39.3</td>
      <td>45.1</td>
      <td>10.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17.2</td>
      <td>45.9</td>
      <td>69.3</td>
      <td>12.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>151.5</td>
      <td>41.3</td>
      <td>58.5</td>
      <td>16.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>180.8</td>
      <td>10.8</td>
      <td>58.4</td>
      <td>17.9</td>
    </tr>
  </tbody>
</table>
</div>



## 데이터셋 살펴보기 

1. 크기 살펴보기 
df.shape

2. 살짝 살펴보기(상관관계)
-height=열 
sns.pairplot(df,x_vars=['',''], y_vars[''], height=4)


```python
print(df.shape)
#(행, 열 )
```

    (200, 4)
    


```python
# Sales와 변수들간의 관계를 보여줌 
sns.pairplot(df,x_vars=['TV','Newspaper','Radio'], y_vars=['Sales'],height=4)
# TV와 Sales간의 선형 회귀를 확인할 수 있음. 
```


![output_8_1](https://user-images.githubusercontent.com/88616282/200857538-b33348be-5698-4f90-a56f-0ec3330e52e7.png)

    


## 데이터셋 가공

input과 output을 항상 지정해줘야함 
**Keras는 np.array로 바꿔준 데이터만 취급한다 


```python
x_data=np.array(df[['TV']],dtype=np.float32)
y_data=np.array(df['Sales'],dtype=np.float32)

print(x_data.shape)
print(y_data.shape)

# y_data를 맞춰줘야함 
```

    (200, 1)
    (200,)
    


```python
# -1남은 수만큼 알아서 변경해라
# 1 뒤에는 무조건 1이 되어야한다 

# 데이터셋 자체가 변하는 것은 아니지만 이래야지 Keras가 인식을 함 

x_data=x_data.reshape((-1,1))
y_data=y_data.reshape((-1,1))

print(x_data.shape)
print(y_data.shape)
```

    (200, 1)
    (200, 1)
    

## 데이터셋 분할
학습 데이터 80% 검증 데이터 20%

실제 실무에서는 학습/검증/테스트 데이터를 나눠야함


```python
# random_state: 섞어서 한다는거 
# 순서대로 값이 나옴 

x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=2021)

print(x_train.shape, x_val.shape)
print(y_train.shape, y_val.shape)

# Training학습 데이터는 160개
# Validation  검증 데이터는 40개 
```

    (160, 1) (40, 1)
    (160, 1) (40, 1)
    

## 학습 방법 

Dense(1) 출력이 1이다 Sales만 예측한다 

한개 Epoch마다 검증을 진행하는거,
Epoch 1/100보면 training loss가 높은걸 확인 

100번째가 되면 loss가 줄어든거를 볼수있음(잘됐다) 



```python
model = Sequential([Dense(1)])

# Adam은 옵티마이저인데 성능이 좋은거 
model.compile(loss='mean_squared_error',optimizer=Adam(lr=0.1))

# 학습은 언제나 fit

model.fit(
    x_train,
    y_train,
    validation_data=(x_val, y_val), # 검증 데이터를 넣어주면 한 epoch이 끝날때마다 자동으로 검증
    epochs=100) # 백번 반복학습해라 

# 순서대로 training loss ->Validation loss 
```

    D:\anaconda\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
      super().__init__(name, **kwargs)
    

    Epoch 1/100
    5/5 [==============================] - 2s 94ms/step - loss: 22966.8496 - val_loss: 11132.6514
    Epoch 2/100
    5/5 [==============================] - 0s 7ms/step - loss: 5388.3115 - val_loss: 843.7969
    Epoch 3/100
    5/5 [==============================] - 0s 8ms/step - loss: 307.7916 - val_loss: 715.0568
    Epoch 4/100
    5/5 [==============================] - 0s 7ms/step - loss: 1469.2754 - val_loss: 2402.9768
    Epoch 5/100
    5/5 [==============================] - 0s 7ms/step - loss: 2020.4226 - val_loss: 1639.5114
    Epoch 6/100
    5/5 [==============================] - 0s 7ms/step - loss: 914.1517 - val_loss: 321.6947
    Epoch 7/100
    5/5 [==============================] - 0s 7ms/step - loss: 108.6977 - val_loss: 41.3750
    Epoch 8/100
    5/5 [==============================] - 0s 7ms/step - loss: 132.1291 - val_loss: 281.5231
    Epoch 9/100
    5/5 [==============================] - 0s 7ms/step - loss: 270.5881 - val_loss: 227.5944
    Epoch 10/100
    5/5 [==============================] - 0s 7ms/step - loss: 141.1764 - val_loss: 43.1888
    Epoch 11/100
    5/5 [==============================] - 0s 7ms/step - loss: 21.6829 - val_loss: 30.5824
    Epoch 12/100
    5/5 [==============================] - 0s 7ms/step - loss: 44.6129 - val_loss: 72.5681
    Epoch 13/100
    5/5 [==============================] - 0s 7ms/step - loss: 53.1169 - val_loss: 43.5014
    Epoch 14/100
    5/5 [==============================] - 0s 7ms/step - loss: 22.9127 - val_loss: 15.9200
    Epoch 15/100
    5/5 [==============================] - 0s 7ms/step - loss: 15.4803 - val_loss: 21.2831
    Epoch 16/100
    5/5 [==============================] - 0s 6ms/step - loss: 21.1734 - val_loss: 20.9890
    Epoch 17/100
    5/5 [==============================] - 0s 7ms/step - loss: 16.7988 - val_loss: 15.4750
    Epoch 18/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.7793 - val_loss: 17.0899
    Epoch 19/100
    5/5 [==============================] - 0s 7ms/step - loss: 14.1743 - val_loss: 18.0329
    Epoch 20/100
    5/5 [==============================] - 0s 7ms/step - loss: 13.8933 - val_loss: 16.0081
    Epoch 21/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.7880 - val_loss: 15.0917
    Epoch 22/100
    5/5 [==============================] - 0s 7ms/step - loss: 13.0264 - val_loss: 15.1351
    Epoch 23/100
    5/5 [==============================] - 0s 7ms/step - loss: 12.9155 - val_loss: 14.9607
    Epoch 24/100
    5/5 [==============================] - 0s 7ms/step - loss: 12.5734 - val_loss: 15.1542
    Epoch 25/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.6047 - val_loss: 15.3123
    Epoch 26/100
    5/5 [==============================] - 0s 7ms/step - loss: 12.5633 - val_loss: 15.0183
    Epoch 27/100
    5/5 [==============================] - 0s 7ms/step - loss: 12.4181 - val_loss: 14.7181
    Epoch 28/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.3857 - val_loss: 14.6185
    Epoch 29/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.3141 - val_loss: 14.6031
    Epoch 30/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.2889 - val_loss: 14.7144
    Epoch 31/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.2158 - val_loss: 14.5243
    Epoch 32/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.1398 - val_loss: 14.3918
    Epoch 33/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.0578 - val_loss: 14.3706
    Epoch 34/100
    5/5 [==============================] - 0s 8ms/step - loss: 12.0034 - val_loss: 14.2820
    Epoch 35/100
    5/5 [==============================] - 0s 8ms/step - loss: 11.9222 - val_loss: 14.1724
    Epoch 36/100
    5/5 [==============================] - 0s 8ms/step - loss: 11.8581 - val_loss: 14.1012
    Epoch 37/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.8241 - val_loss: 14.0131
    Epoch 38/100
    5/5 [==============================] - 0s 8ms/step - loss: 11.7394 - val_loss: 14.0242
    Epoch 39/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.6743 - val_loss: 13.9438
    Epoch 40/100
    5/5 [==============================] - 0s 8ms/step - loss: 11.7327 - val_loss: 13.7185
    Epoch 41/100
    5/5 [==============================] - 0s 8ms/step - loss: 11.6229 - val_loss: 13.6874
    Epoch 42/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.5133 - val_loss: 13.7707
    Epoch 43/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.4573 - val_loss: 13.5594
    Epoch 44/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.3591 - val_loss: 13.4134
    Epoch 45/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.2992 - val_loss: 13.4055
    Epoch 46/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.2109 - val_loss: 13.3196
    Epoch 47/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.1610 - val_loss: 13.2649
    Epoch 48/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.1060 - val_loss: 13.0999
    Epoch 49/100
    5/5 [==============================] - 0s 7ms/step - loss: 11.0394 - val_loss: 13.0742
    Epoch 50/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.9747 - val_loss: 12.9760
    Epoch 51/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.9042 - val_loss: 12.9913
    Epoch 52/100
    5/5 [==============================] - 0s 8ms/step - loss: 10.8278 - val_loss: 12.8607
    Epoch 53/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.7701 - val_loss: 12.6917
    Epoch 54/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.7025 - val_loss: 12.6389
    Epoch 55/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.6289 - val_loss: 12.5637
    Epoch 56/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.5557 - val_loss: 12.5386
    Epoch 57/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.4954 - val_loss: 12.4455
    Epoch 58/100
    5/5 [==============================] - 0s 8ms/step - loss: 10.4625 - val_loss: 12.2868
    Epoch 59/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.3904 - val_loss: 12.2883
    Epoch 60/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.3172 - val_loss: 12.1606
    Epoch 61/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.2690 - val_loss: 12.0462
    Epoch 62/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.1760 - val_loss: 12.0022
    Epoch 63/100
    5/5 [==============================] - 0s 6ms/step - loss: 10.0953 - val_loss: 12.0362
    Epoch 64/100
    5/5 [==============================] - 0s 7ms/step - loss: 10.1158 - val_loss: 11.9957
    Epoch 65/100
    5/5 [==============================] - 0s 7ms/step - loss: 9.9669 - val_loss: 11.7217
    Epoch 66/100
    5/5 [==============================] - 0s 8ms/step - loss: 9.9226 - val_loss: 11.6364
    Epoch 67/100
    5/5 [==============================] - 0s 7ms/step - loss: 9.8758 - val_loss: 11.5577
    Epoch 68/100
    5/5 [==============================] - 0s 7ms/step - loss: 9.8058 - val_loss: 11.5649
    Epoch 69/100
    5/5 [==============================] - 0s 8ms/step - loss: 9.7348 - val_loss: 11.5505
    Epoch 70/100
    5/5 [==============================] - 0s 7ms/step - loss: 9.7052 - val_loss: 11.3821
    Epoch 71/100
    5/5 [==============================] - 0s 7ms/step - loss: 9.6129 - val_loss: 11.2982
    Epoch 72/100
    5/5 [==============================] - 0s 8ms/step - loss: 9.7519 - val_loss: 11.1538
    Epoch 73/100
    5/5 [==============================] - 0s 8ms/step - loss: 9.5079 - val_loss: 11.3153
    Epoch 74/100
    5/5 [==============================] - 0s 8ms/step - loss: 9.5112 - val_loss: 11.1372
    Epoch 75/100
    5/5 [==============================] - 0s 8ms/step - loss: 9.3935 - val_loss: 10.9507
    Epoch 76/100
    5/5 [==============================] - 0s 8ms/step - loss: 9.3549 - val_loss: 10.8708
    Epoch 77/100
    5/5 [==============================] - 0s 7ms/step - loss: 9.2369 - val_loss: 10.9543
    Epoch 78/100
    5/5 [==============================] - 0s 7ms/step - loss: 9.2733 - val_loss: 10.8961
    Epoch 79/100
    5/5 [==============================] - 0s 8ms/step - loss: 9.1324 - val_loss: 10.6408
    Epoch 80/100
    5/5 [==============================] - 0s 8ms/step - loss: 9.1211 - val_loss: 10.6055
    Epoch 81/100
    5/5 [==============================] - 0s 7ms/step - loss: 9.0031 - val_loss: 10.5851
    Epoch 82/100
    5/5 [==============================] - 0s 8ms/step - loss: 8.9614 - val_loss: 10.5188
    Epoch 83/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.9409 - val_loss: 10.3829
    Epoch 84/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.8457 - val_loss: 10.3533
    Epoch 85/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.9000 - val_loss: 10.2123
    Epoch 86/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.7539 - val_loss: 10.3317
    Epoch 87/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.7319 - val_loss: 10.2432
    Epoch 88/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.6851 - val_loss: 10.0020
    Epoch 89/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.5874 - val_loss: 9.9874
    Epoch 90/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.5258 - val_loss: 10.0025
    Epoch 91/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.5226 - val_loss: 9.8952
    Epoch 92/100
    5/5 [==============================] - 0s 6ms/step - loss: 8.4166 - val_loss: 9.7370
    Epoch 93/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.4828 - val_loss: 9.7858
    Epoch 94/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.3643 - val_loss: 9.6268
    Epoch 95/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.2727 - val_loss: 9.6547
    Epoch 96/100
    5/5 [==============================] - 0s 8ms/step - loss: 8.2562 - val_loss: 9.5811
    Epoch 97/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.3078 - val_loss: 9.4396
    Epoch 98/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.1611 - val_loss: 9.4716
    Epoch 99/100
    5/5 [==============================] - 0s 7ms/step - loss: 8.0886 - val_loss: 9.4966
    Epoch 100/100
    5/5 [==============================] - 0s 7ms/step - loss: 7.9975 - val_loss: 9.2438
    




    <keras.callbacks.History at 0x1e93943cdc0>



## 검증 데이터로 예측 


```python
y_pred=model.predict(x_val)

plt.scatter(x_val,y_val)
plt.scatter(x_val,y_pred,color='r') #예측값은 빨간색. 가설은 선형이라서 
plt.show()
```

    2/2 [==============================] - 0s 0s/step
    


    ![output_17_1](https://user-images.githubusercontent.com/88616282/200857188-73ff77d7-c3c5-43da-921e-0c78e1fd61e5.png)




## 다중선형회귀(Multi-variable linear regression)

아까전에는 Tv~Sales만 봤다면 이번에는 한번에 TV,Newspaper,Radio와 Sales를 봄 


```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam, SGD
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.model_selection import train_test_split
#===============================================================================
df=pd.read_csv("D:/anaconda/00.studying/advertising-dataset/advertising.csv")

#===============================================================================
x_data = np.array(df[['TV', 'Newspaper', 'Radio']], dtype=np.float32)
y_data = np.array(df['Sales'], dtype=np.float32)
#===============================================================================
# x_data이게 뒤에 값이 1개가 아니라 3개라서 
x_data=x_data.reshape((-1,3))
y_data=y_data.reshape((-1,1))

print(x_data.shape) #(200,3)
print(y_data.shape) #(200,1)
#===============================================================================
x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=2021)

print(x_train.shape, x_val.shape)
print(y_train.shape, y_val.shape)

#===============================================================================
model = Sequential([Dense(1)])
model.compile(loss='mean_squared_error',optimizer=Adam(lr=0.1))

model.fit(
    x_train,
    y_train,
    validation_data=(x_val, y_val),
    epochs=100)

# 나오는 값은 다를수 있음
# initial weight를 랜덤으로 반영되고 최적화시 변동이 있을수있어서 
```

    (200, 3)
    (200, 1)
    (160, 3) (40, 3)
    (160, 1) (40, 1)
    Epoch 1/100
    

    D:\anaconda\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
      super().__init__(name, **kwargs)
    

    5/5 [==============================] - 1s 34ms/step - loss: 513.3303 - val_loss: 246.8809
    Epoch 2/100
    5/5 [==============================] - 0s 6ms/step - loss: 211.6483 - val_loss: 104.9799
    Epoch 3/100
    5/5 [==============================] - 0s 7ms/step - loss: 82.1085 - val_loss: 40.4558
    Epoch 4/100
    5/5 [==============================] - 0s 8ms/step - loss: 23.2890 - val_loss: 17.7698
    Epoch 5/100
    5/5 [==============================] - 0s 7ms/step - loss: 12.0168 - val_loss: 20.6800
    Epoch 6/100
    5/5 [==============================] - 0s 7ms/step - loss: 18.2963 - val_loss: 20.1757
    Epoch 7/100
    5/5 [==============================] - 0s 7ms/step - loss: 18.8005 - val_loss: 18.2845
    Epoch 8/100
    5/5 [==============================] - 0s 7ms/step - loss: 13.5975 - val_loss: 11.9422
    Epoch 9/100
    5/5 [==============================] - 0s 8ms/step - loss: 7.7854 - val_loss: 8.2781
    Epoch 10/100
    5/5 [==============================] - 0s 6ms/step - loss: 5.2677 - val_loss: 8.1238
    Epoch 11/100
    5/5 [==============================] - 0s 8ms/step - loss: 5.3770 - val_loss: 6.7330
    Epoch 12/100
    5/5 [==============================] - 0s 7ms/step - loss: 5.0916 - val_loss: 7.0481
    Epoch 13/100
    5/5 [==============================] - 0s 7ms/step - loss: 5.3677 - val_loss: 6.8456
    Epoch 14/100
    5/5 [==============================] - 0s 7ms/step - loss: 4.6662 - val_loss: 6.3403
    Epoch 15/100
    5/5 [==============================] - 0s 7ms/step - loss: 4.1484 - val_loss: 6.0778
    Epoch 16/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.9642 - val_loss: 6.3281
    Epoch 17/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.8407 - val_loss: 5.9018
    Epoch 18/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.8200 - val_loss: 5.6761
    Epoch 19/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.6902 - val_loss: 5.8415
    Epoch 20/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.9959 - val_loss: 5.5057
    Epoch 21/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.7407 - val_loss: 5.3393
    Epoch 22/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.4210 - val_loss: 5.2209
    Epoch 23/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.3558 - val_loss: 5.4873
    Epoch 24/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.7211 - val_loss: 5.0557
    Epoch 25/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.4418 - val_loss: 5.1545
    Epoch 26/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.3102 - val_loss: 4.9271
    Epoch 27/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.2041 - val_loss: 5.1066
    Epoch 28/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.7887 - val_loss: 5.3850
    Epoch 29/100
    5/5 [==============================] - 0s 8ms/step - loss: 4.3765 - val_loss: 4.6566
    Epoch 30/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.4047 - val_loss: 4.6399
    Epoch 31/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.0458 - val_loss: 4.5184
    Epoch 32/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.0295 - val_loss: 4.5857
    Epoch 33/100
    5/5 [==============================] - 0s 8ms/step - loss: 2.9997 - val_loss: 4.6174
    Epoch 34/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.0120 - val_loss: 4.3314
    Epoch 35/100
    5/5 [==============================] - 0s 7ms/step - loss: 2.9759 - val_loss: 4.3921
    Epoch 36/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.0887 - val_loss: 4.3755
    Epoch 37/100
    5/5 [==============================] - 0s 9ms/step - loss: 2.8916 - val_loss: 4.3571
    Epoch 38/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.0922 - val_loss: 4.1381
    Epoch 39/100
    5/5 [==============================] - 0s 7ms/step - loss: 2.9762 - val_loss: 4.8181
    Epoch 40/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.1646 - val_loss: 5.0439
    Epoch 41/100
    5/5 [==============================] - 0s 8ms/step - loss: 3.0295 - val_loss: 5.3878
    Epoch 42/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.7767 - val_loss: 6.6289
    Epoch 43/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.8362 - val_loss: 6.5160
    Epoch 44/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.5519 - val_loss: 7.4380
    Epoch 45/100
    5/5 [==============================] - 0s 7ms/step - loss: 4.4062 - val_loss: 5.6620
    Epoch 46/100
    5/5 [==============================] - 0s 7ms/step - loss: 4.0160 - val_loss: 4.2143
    Epoch 47/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.8364 - val_loss: 4.2148
    Epoch 48/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.6200 - val_loss: 4.9417
    Epoch 49/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.4471 - val_loss: 4.6651
    Epoch 50/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.3241 - val_loss: 4.4125
    Epoch 51/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.9000 - val_loss: 4.4085
    Epoch 52/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.9405 - val_loss: 4.3599
    Epoch 53/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.8423 - val_loss: 4.1874
    Epoch 54/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.9197 - val_loss: 7.0984
    Epoch 55/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.1940 - val_loss: 4.1455
    Epoch 56/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.5761 - val_loss: 5.1857
    Epoch 57/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.8857 - val_loss: 4.4562
    Epoch 58/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.1598 - val_loss: 4.1995
    Epoch 59/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.8243 - val_loss: 4.0718
    Epoch 60/100
    5/5 [==============================] - 0s 7ms/step - loss: 2.7695 - val_loss: 3.9518
    Epoch 61/100
    5/5 [==============================] - 0s 7ms/step - loss: 2.6281 - val_loss: 3.9291
    Epoch 62/100
    5/5 [==============================] - 0s 7ms/step - loss: 2.8495 - val_loss: 4.8371
    Epoch 63/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.7682 - val_loss: 10.1013
    Epoch 64/100
    5/5 [==============================] - 0s 6ms/step - loss: 5.0973 - val_loss: 6.6686
    Epoch 65/100
    5/5 [==============================] - 0s 7ms/step - loss: 4.8218 - val_loss: 4.4760
    Epoch 66/100
    5/5 [==============================] - 0s 6ms/step - loss: 5.1035 - val_loss: 4.1094
    Epoch 67/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.3029 - val_loss: 4.8054
    Epoch 68/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.4387 - val_loss: 3.6337
    Epoch 69/100
    5/5 [==============================] - 0s 7ms/step - loss: 3.0356 - val_loss: 3.7051
    Epoch 70/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.4384 - val_loss: 4.3108
    Epoch 71/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.0007 - val_loss: 4.3454
    Epoch 72/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.4621 - val_loss: 5.0456
    Epoch 73/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.7633 - val_loss: 5.1636
    Epoch 74/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.2942 - val_loss: 5.8778
    Epoch 75/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.6258 - val_loss: 7.6580
    Epoch 76/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.7079 - val_loss: 5.2658
    Epoch 77/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.3874 - val_loss: 8.8409
    Epoch 78/100
    5/5 [==============================] - 0s 6ms/step - loss: 5.3753 - val_loss: 6.1720
    Epoch 79/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.7052 - val_loss: 3.6905
    Epoch 80/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.7019 - val_loss: 3.7648
    Epoch 81/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.7089 - val_loss: 3.9585
    Epoch 82/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.9707 - val_loss: 3.9830
    Epoch 83/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.3497 - val_loss: 3.9006
    Epoch 84/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.9239 - val_loss: 5.2271
    Epoch 85/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.8736 - val_loss: 4.1547
    Epoch 86/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.0001 - val_loss: 3.7143
    Epoch 87/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.8705 - val_loss: 4.1023
    Epoch 88/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.0492 - val_loss: 3.7894
    Epoch 89/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.7787 - val_loss: 3.9185
    Epoch 90/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.2603 - val_loss: 3.7243
    Epoch 91/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.4769 - val_loss: 4.0368
    Epoch 92/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.9992 - val_loss: 4.8614
    Epoch 93/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.5151 - val_loss: 10.3857
    Epoch 94/100
    5/5 [==============================] - 0s 7ms/step - loss: 4.3587 - val_loss: 6.3025
    Epoch 95/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.2368 - val_loss: 6.7931
    Epoch 96/100
    5/5 [==============================] - 0s 6ms/step - loss: 5.8745 - val_loss: 5.7916
    Epoch 97/100
    5/5 [==============================] - 0s 6ms/step - loss: 5.0652 - val_loss: 3.9131
    Epoch 98/100
    5/5 [==============================] - 0s 6ms/step - loss: 4.3887 - val_loss: 3.8016
    Epoch 99/100
    5/5 [==============================] - 0s 6ms/step - loss: 2.8208 - val_loss: 4.8754
    Epoch 100/100
    5/5 [==============================] - 0s 6ms/step - loss: 3.6151 - val_loss: 4.2103
    




    <keras.callbacks.History at 0x1e93e32f670>




```python
y_pred=model.predict(x_val)
print(y_pred.shape)
```

    2/2 [==============================] - 0s 997us/step
    (40, 1)
    

## TV예측 그래프 

차원을 높여서 보면 선형으로 보일꺼임

빨간색과 파란색의 분포가 비슷한걸로 보아 예측이 잘됨 


```python
plt.scatter(x_val[:, 0], y_val)
plt.scatter(x_val[:, 0], y_pred, color='r')
plt.show()
```


    
![output_22_0](https://user-images.githubusercontent.com/88616282/200857254-711bbeb0-0862-40ba-b972-12d93f46d615.png)



## Newspaper 예측 그래프 


```python
plt.scatter(x_val[:, 1], y_val)
plt.scatter(x_val[:, 1], y_pred, color='r')
plt.show()
```


    
![output_24_0](https://user-images.githubusercontent.com/88616282/200857290-917c2071-d812-4497-b142-c8ffc179a913.png)

    


## Radio 예측 그래프 


```python
plt.scatter(x_val[:, 2], y_val)
plt.scatter(x_val[:, 2], y_pred, color='r')
plt.show()
```

    
![output_26_0](https://user-images.githubusercontent.com/88616282/200857314-7d455d19-d1da-42c1-9d62-201019d6a248.png)


