#  변수 축소 
## 주성분분석
<br>
<b>가.</b> 주성분분석의 개념<br>

* 주성분분석이란 데이터에 여러 변수들이 있을 때, 서로 상관성이 높은 변수들의 선형결합으로 이루어진 "주성분"이라는 새로운 변수를 만들어 변수들을 요약하고 축소하는 기법 
<br>
예를 들어서 변수들중에서 10개를 주성분으로 했을때 이게 전체의 몇퍼센트를 설명하는지 
<br>

<b>나.</b>  파이썬을 이용한 주성분 분석
* 변수간의 스케일이 차이가 나면 스케일 큰 변수가 주성분에 영향을 많이 주기 때문에 주성분 분석 전에 변수를 표준화나 
  정규화시켜주는 것이 좋다. 데이터 표준화에 사용하는 함수는 StandardScaler()이다. 
* 파이썬에서 주성분분석을 수행할 수 있는 기본적인 함수는 sklearn의 PCA함수이다. 
* pca.explained_variance_ratio_를 통해서 또는 Scree Plot으로 주성분의 설명력을 확인 할 수 있다. 
* PCA함수는 공분산행렬의 고유벡터를 구하는 방법을 사용한다. 



## 차원을 줄여야 하는 이유 
![image](./차원의저주.jpg)

* 데이터 용량이 커질수록 불필요한 샘플 증가 현상이 일어남 
* 과도한 정보를 훈련시키면 부정확하고 잘못된 모델이 탄생할 가능성이 증가함 

==> 차원 축소로 해결 가능 

<br>
* 함수 사용예제 


```python
import pandas as pd
```


```python
Iris_data = pd.read_csv('../../data/iris.csv')

#수치형 데이터만 추출
features = ['sepal length', 'sepal width', 'petal length', 'petal width']
x = Iris_data.loc[:, features].values

#수치형 변수 정규화
from sklearn.preprocessing import StandardScaler
x = StandardScaler().fit_transform(x)
```


```python
from sklearn.decomposition import PCA
## Scree Plot 으로 주성분 개수 정하는 방법 
pca = PCA(n_components=4)
pca_fit = pca.fit(x)
```


```python
pca_fit.explained_variance_ratio_
```




    array([0.72770452, 0.23030523, 0.03683832, 0.00515193])




```python
print("\n ==================== PCA Result Summary ===================")
print("\n고유 값 : \n",pca.singular_values_)
print("\n분산 설명력: \n", pca.explained_variance_ratio_)
```

    
     ==================== PCA Result Summary ===================
    
    고유 값 : 
     [20.89551896 11.75513248  4.7013819   1.75816839]
    
    분산 설명력: 
     [0.72770452 0.23030523 0.03683832 0.00515193]
    


```python
features
```




    ['sepal length', 'sepal width', 'petal length', 'petal width']




```python
# Scree Plot 
import matplotlib.pyplot as plt
plt.title('Scree Plot')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.plot(pca.explained_variance_ratio_,'o-')
```




    [<matplotlib.lines.Line2D at 0x18dfccc7308>]




    
![png](output_9_1.png)
    



```python
pca = PCA(n_components=2) #PCA 객체 생성 (주성분 갯수 2개 생성)
principalComponents = pca.fit_transform(x) ## 주성분 2개 객체에 할당 
principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component1', 'principal component2']) 
pca.explained_variance_ratio_
```




    array([0.72770452, 0.23030523])




```python
principalDf
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>principal component1</th>
      <th>principal component2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-2.264542</td>
      <td>0.505704</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-2.086426</td>
      <td>-0.655405</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-2.367950</td>
      <td>-0.318477</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-2.304197</td>
      <td>-0.575368</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-2.388777</td>
      <td>0.674767</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>1.870522</td>
      <td>0.382822</td>
    </tr>
    <tr>
      <th>146</th>
      <td>1.558492</td>
      <td>-0.905314</td>
    </tr>
    <tr>
      <th>147</th>
      <td>1.520845</td>
      <td>0.266795</td>
    </tr>
    <tr>
      <th>148</th>
      <td>1.376391</td>
      <td>1.016362</td>
    </tr>
    <tr>
      <th>149</th>
      <td>0.959299</td>
      <td>-0.022284</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 2 columns</p>
</div>




```python
import matplotlib.pyplot as plt

fig = plt.figure(figsize = (8, 8))
ax = fig.add_subplot(1, 1, 1)
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize=20)

targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']
colors = ['r', 'g', 'b']
for target, color in zip(targets,colors):
    indicesToKeep = Iris_data['target'] == target
    ax.scatter(principalDf.loc[indicesToKeep, 'principal component1']
               , principalDf.loc[indicesToKeep, 'principal component2']
               , c = color
               , s = 50)
ax.legend()
ax.grid()
```

    No handles with labels found to put in legend.
    
![output_17_1](https://user-images.githubusercontent.com/88616282/206831336-6967fa2a-345d-4d7d-84a1-6d2c1fa1f374.png)

